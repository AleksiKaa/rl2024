{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8a4eb5b52572512594ce9f767e784ef",
     "grade": false,
     "grade_id": "cell-4b5bc7c2e51c6d03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 3 - Q Learning </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2024 - Nov 30, 2024</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Cartpole </a>\n",
    "* <a href='#2.'> 3. Lunar Lander </a> \n",
    "* <a href='#4.'> 4. Submitting </a>\n",
    "* <a href='#4.1'> 4.1 Feedback </a>\n",
    "* <a href='#5.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Q-Learning  (25 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Visualizing the Value Function (10 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 2.1</b> Analyzing the Value Function Heatmap (15 points) </a>\\\n",
    "<a href='#T3'><b>Student Task 3.</b> Investigating Initial Values (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 3.1</b> Analyzing Initial Values (5 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 3.2</b> Exploration (15 points) </a>\\\n",
    "<a href='#T4'><b>Student Task 4.</b> Using Q-Learning on the Lunar Lander Environment (5 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 4.1</b> Lunar Lander Performance (15 points) </a>\n",
    "\n",
    "**Total Points:** 100\n",
    "\n",
    "**Estimated runtime of all the cells:** 2 hour "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845acab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b83672bf7f3be091f160f135ae78d22c",
     "grade": false,
     "grade_id": "cell-0ecccf891e2408d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In this exercise, you'll be applying grid-based Q-learning to the **Cartpole** and **LunarLander** environments.\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- To understand and develop intuition about Q-learning.\n",
    "- To understand simple exploration methods like GLIE and how exploration effects learning\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "You don‚Äôt have to edit any other file other than ```ex3.ipynb``` to complete this exercise.\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                           # Config files for environments e.g. define the maximum number of steps in an episode.\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                          # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄCartPole-v1\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                  # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îÇ   logging.pkl            # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_e.pkl         # Contains qtable data for epsilon fixed\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_glie.pkl      # Contains qtable data for glie\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *e.png                 # Contains training performance plot for epsilon fixed\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *glie.png              # Contains training performance plot for glie\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄLunarLander-v2\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo\n",
    "‚îÇ   ‚îÇ   ‚îÇ   logging.pkl\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_e.pkl\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *q_table_glie.pkl\n",
    "‚îÇ   ‚îÇ   ‚îÇ   *e.png                 \n",
    "‚îÇ   ‚îÇ   ‚îÇ   *glie.png              \n",
    "‚îÇ   ex3.ipynb                      # Main assignment file containing tasks <---------\n",
    "‚îÇ   setup.py                       # Contains setup function\n",
    "‚îÇ   utils.py                       # Contains useful functions \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36c86f",
   "metadata": {},
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don‚Äôt copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c999cc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15066d1f084603d0367d637a21c41cef",
     "grade": false,
     "grade_id": "cell-c2117ccc1c4199d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 2. Cartpole <a id='2.'></a>\n",
    "\n",
    "Recall the Cartpole environment from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfe17b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2524f28d2ba6329fe80ca41bedc61ea5",
     "grade": false,
     "grade_id": "cell-83fe844e90e20c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Q-Learning  (25 points) </h3> \n",
    "\n",
    "Implement Q-learning as presented in [Sutton and Barto, 2017, Section 6.5][1] for the Cartpole environment. Once you've successfully implemented the `get_action` and `update_q_value` functions,\n",
    "you can compare the two exploration methods:\n",
    "\n",
    "1. Use a constant value of $\\epsilon = 0.1$:\n",
    "    \n",
    "- Training:\n",
    "    ```python\n",
    "    # Training with constant epsilon 0.1\n",
    "    cfg_args = dict(epsilon=0.1, save_video=False)\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)\n",
    "    ```\n",
    "- Testing:\n",
    "    ```python\n",
    "    # Testing with constant epsilon 0.1\n",
    "    cfg_args=dict(epsilon=0.1, save_video=True)\n",
    "    test(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)\n",
    "    ```\n",
    "- Visualization: To record and visualize the agent's performance, you could set `save_video=True` and run the following:\n",
    "\n",
    "    ```python\n",
    "    Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex3-episode-0.mp4', embed=True, html_attributes=\"loop autoplay\")\n",
    "    ```\n",
    "\n",
    "2. Implement **GLIE** (i.e., *Greedy in the Limit with Infinite Exploration*), which reduces the value of $\\epsilon$ over time. For details on the formula, refer to **[Lecture 3]**.\n",
    "    \n",
    "- Training (Ensure you've determined the correct value for `glie_b`):  \n",
    "    ```python\n",
    "    # Training with GLIE\n",
    "    cfg_args=dict(epsilon='glie', glie_b=<insert-correct-value>, save_video=False) # insert correct value for glie_b\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args) # < 30 mins\n",
    "    ```\n",
    "- Testing:\n",
    " \n",
    "    ```python\n",
    "    # Testing with GLIE\n",
    "    cfg_args=dict(epsilon='glie', save_video=True)\n",
    "    test(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)\n",
    "    ```\n",
    "- Visualization: To record and visualize the agent's performance, you could set `save_video=True` and run the following:\n",
    "\n",
    "    ```python\n",
    "    Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex3-episode-0.mp4', embed=True, html_attributes=\"loop autoplay\")\n",
    "    ```\n",
    "    \n",
    "These videos will be stored in the **results** folder.\n",
    "\n",
    "**Note:** Before you proceed with the \"Training with GLIE\" method, ensure you have completed the \"Training with a constant epsilon\" steps. Since the video save path is the same, the results will be overwritten.\n",
    "\n",
    "---\n",
    "\n",
    "**Please attach the following files to your submission:**\n",
    "\n",
    "- Training performance plots:\n",
    "  - `task1_e.png`: This file represents the episode versus the smoothed episodic reward for the constant exploration rate.\n",
    "  - `task1_glie.png`: This file represents the episode versus the smoothed episodic reward for GLIE.\n",
    "\n",
    "- Q-table files:\n",
    "  - `task1_q_table_e.pkl`: Q-table for the constant exploration rate.\n",
    "  - `task1_q_table_glie.pkl`: Q-table for GLIE.\n",
    "\n",
    "Ensure all files are correctly named and included.\n",
    "\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/ep_reward.png\" width=\"600px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The training performance plot should look similar to the one presented here when using GLIE  </figcaption>\n",
    "</figure>\n",
    "\n",
    "### References\n",
    "[1]: http://incompleteideas.net/book/RLbook2018.pdf\n",
    "[Sutton and Barto, 2017]: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (in progress).\" London, England (2017).\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4375e55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1eacb1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92d6d673e50350c2b0b437bc2a2803a9",
     "grade": true,
     "grade_id": "cell-6d789f0e8b41dcd4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d6ce86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- Third-party Libraries ---\n",
    "import numpy as np  # Fundamental package for scientific computing\n",
    "from IPython.display import Video  # For displaying videos in the notebook\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Custom Modules ---\n",
    "from setup import setup  # Setup functions for training\n",
    "import utils as u  # Helper functions for the notebook\n",
    "\n",
    "# --- Configurations ---\n",
    "# Set working directory to 'results'\n",
    "work_dir = Path().cwd() / 'results'\n",
    "\n",
    "# Suppress DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91287ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_q_table(observation_space, action_dim, discr, bool_position=None, init_q=0.):\n",
    "    \"\"\"\n",
    "    Initialize a Q-table for the given observation space and actions.\n",
    "    \n",
    "    Parameters:\n",
    "    - observation_space: The state space of the environment.\n",
    "    - action_dim: The number of possible actions.\n",
    "    - discr: The number of divisions for discretization.\n",
    "    - bool_position: Indexes where the observation space has boolean values.\n",
    "    - init_q: Initial value for the Q-table.\n",
    "    \n",
    "    Returns:\n",
    "    - axis: A list of numpy arrays representing the discrete states. \n",
    "            Each array's length corresponds to either 'discr' or 2 (for boolean values).\n",
    "    - q_table: The initialized Q-table. Its shape is determined by [*axis dimensions, action_dim].\n",
    "               For example, if the axis dimensions were [10, 10] and action_dim was 3, \n",
    "               the q_table shape would be [10, 10, 3].\n",
    "    \"\"\"\n",
    "    high_values = observation_space.high\n",
    "    low_values = observation_space.low\n",
    "    axis = []\n",
    "    for idx, (low_val, high_val) in enumerate(zip(low_values, high_values)):\n",
    "        # here to avoid inf boundary, we truncate the value to [-4, 4] \n",
    "        if low_val < -1e10: low_val = -4\n",
    "        if high_val > 1e10: high_val = 4\n",
    "        \n",
    "        if (bool_position is not None) and (idx in bool_position):\n",
    "            axis.append(np.linspace(low_val, high_val, 2, dtype=np.float32)) # for boolean, we only have two values: 1., 0.\n",
    "        else:\n",
    "            axis.append(np.linspace(low_val, high_val, discr, dtype=np.float32))\n",
    "    _shape = [ax.shape[0] for ax in axis] + [action_dim]\n",
    "    q_table = np.zeros(_shape) + init_q\n",
    "    return axis, q_table\n",
    "\n",
    "def get_table_idx(state, axis):\n",
    "    \"\"\"\n",
    "    Give a state, discretize it, and return the index in each dimension (axis). \n",
    "    With the returned index, you can access q(s,.) with q_table[idx].\n",
    "    \n",
    "    Parameters:\n",
    "    - state: The state to be discretized. \n",
    "    - axis: The discrete state space as a list of numpy arrays. \n",
    "    \n",
    "    Returns:\n",
    "    - idx: A tuple representing the index of the discretized state in the Q-table. \n",
    "           Its shape matches the dimensions of the provided 'axis'.\n",
    "    \"\"\"\n",
    "    def _get_ax_idx(ax, value):\n",
    "        return np.argmin(np.abs(ax - value))\n",
    "    return tuple([_get_ax_idx(ax, value) for ax, value in zip(axis, state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b139b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_action(state, q_axis, q_table, epsilon=0.0):\n",
    "    \"\"\"\n",
    "    Determine the action to take based on epsilon-greedy strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    - state: The current state of the agent.\n",
    "    - q_axis: The discrete state space as a list of numpy arrays.\n",
    "    - q_table: The Q-table with learned values.\n",
    "    - epsilon: The probability with which a random action is taken (exploration). \n",
    "               Default is 0 (purely greedy).\n",
    "    \n",
    "    Returns:\n",
    "    - action: The chosen action, either greedily or randomly based on epsilon.\n",
    "    \"\"\"\n",
    "    # if epsilon == 0.0, the policy will be greedy -- always choose the best action\n",
    "    '''\n",
    "    # TODO: Task 1, implement epsilon-greedy\n",
    "    # 1. Discretize the given state by using the get_table_idx(state, axis) function\n",
    "    # 2. Use the discretized state's index to access the Q-table\n",
    "    # 3. Generate a random number between 0 and 1.\n",
    "    # 4. Compare this number to `epsilon`\n",
    "    # 5. If the random number is less than `epsilon`, choose a random action.\n",
    "    # 6. Otherwise, choose the best action (greedy action)\n",
    "    '''\n",
    "    ########## Your code starts here ##########\n",
    "    \n",
    "    discrete_state = get_table_idx(state, q_axis)\n",
    "    action_values = q_table[discrete_state]\n",
    "    \n",
    "    action = np.argmax(action_values)\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        # Get actions\n",
    "        actions = np.arange(action_values.size)\n",
    "        # Remove the best action\n",
    "        actions = np.delete(actions, action)\n",
    "        # Pick random action\n",
    "        action = np.random.choice(actions)\n",
    "    \n",
    "    ########## Your code ends here #########\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_q_value(old_state, action, new_state, gamma, reward, done, alpha, q_axis, q_table):\n",
    "    \"\"\"\n",
    "    Update the Q-value for a state-action pair based on the Q-learning update rule.\n",
    "    \n",
    "    Parameters:\n",
    "    - old_state: The original state before taking the action.\n",
    "    - action: The action taken.\n",
    "    - new_state: The state after taking the action.\n",
    "    - gamma: Discount factor.\n",
    "    - reward: Immediate reward for taking the action.\n",
    "    - done: Boolean indicating if the episode is done.\n",
    "    - alpha: Learning rate.\n",
    "    - q_axis: The discrete state space as a list of numpy arrays.\n",
    "    - q_table: The Q-table with learned values.\n",
    "    \n",
    "    Returns:\n",
    "    - q_table: Updated Q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    # q_table contains discretized state-action space (16, 16, 16, 16, 2) values\n",
    "    # Each observation is discritized 16 times and the last dimension are the two\n",
    "    # actions that can be taken. \n",
    "    '''\n",
    "    # TODO: Task 1, update q value\n",
    "    # 1. Retrieve Old Q-Value\n",
    "    # 2. Determine the Next Action\n",
    "    # 3. Retrieve New Q-Value\n",
    "    #     If the episode isn't done, get the Q-value for the new_state and next_action.\n",
    "    #     If the episode is done, set the new Q-value to zero.\n",
    "    # 4. Compute the Updated Q-Value\n",
    "    # 5. Update the Q-Table \n",
    "    # 6. Return the Updated Q-Table\n",
    "    '''\n",
    "    old_table_idx = get_table_idx(old_state, q_axis) # idx of q(s_old, *)\n",
    "    new_table_idx = get_table_idx(new_state, q_axis) # idx of q(s_new, *)  \n",
    "    ########## Your code starts here ########## \n",
    "    \n",
    "    old_v = q_table[old_table_idx][action]\n",
    "    next_action = get_action(new_state, q_axis, q_table)\n",
    "    #new_v = q_table[new_table_idx][next_action]\n",
    "    \n",
    "    if done:\n",
    "        new_v = 0\n",
    "    else:\n",
    "        new_v = q_table[new_table_idx][next_action]\n",
    "    \n",
    "    new_value = old_v + alpha * (reward + gamma * new_v - old_v)    \n",
    "    q_table[old_table_idx][action] = new_value\n",
    "    \n",
    "    ########### Your code ends here ########## \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0d958b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc16991d5ec22369589192a57b098e56",
     "grade": true,
     "grade_id": "cell-b84fd8ddc27262ad",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the get_action function \n",
    "\n",
    "def test_get_action():\n",
    "    test_state = [1,0]\n",
    "    test_qaxis = np.array([-4.,3.]), np.array([-4. ,-6. ])\n",
    "\n",
    "    test_qtable = np.array([[[0.49364422, 0.83209827],[0.987027,  0.48691833]],[[0.40560464, 0.26888288],[0.90825581, 0.88758134]]])\n",
    "    assert get_action(test_state, test_qaxis, test_qtable, epsilon=0) == 0\n",
    "    test_qtable = np.array([[[0.49364422, 0.83209827],[0.987027,  0.48691833]],[[-0.40560464, 0.26888288],[0.90825581, 0.88758134]]])\n",
    "    assert get_action(test_state, test_qaxis, test_qtable, epsilon=0) == 1\n",
    "\n",
    "\n",
    "# test update_q_value function\n",
    "def test_update_q_value():\n",
    "    test_state = [1,0]\n",
    "    test_qaxis = np.array([-4.,3.]), np.array([-4. ,-6. ])\n",
    "    test_qtable = np.array([[[0.49364422, 0.83209827],[0.987027,  0.48691833]],[[-0.40560464, 0.26888288],[0.90825581, 0.88758134]]])\n",
    "    new_test_state = [0,1]\n",
    "    reward = 1\n",
    "    gamma = 0.9\n",
    "    done = False\n",
    "    action = 0\n",
    "    alpha = 1\n",
    "    updated_q_value = update_q_value(test_state, action, new_test_state, gamma, reward, done, alpha, test_qaxis, test_qtable)\n",
    "    assert np.allclose(updated_q_value, np.array([[[0.49364422, 0.83209827],[0.987027  , 0.48691833]],[[1.24199459, 0.26888288],[0.90825581, 0.88758134]]]), 0.001)\n",
    "\n",
    "test_get_action()\n",
    "test_update_q_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62297b95-5f55-4dcc-827b-ff984ae99bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_data(training_data, save_path):\n",
    "    df = pd.DataFrame(training_data)\n",
    "    plt.figure(figsize=(5, 3.5))\n",
    "    sns.lineplot(data=df, x='episode', y='ep_reward_avg')\n",
    "    plt.title('Training Performance')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def train(cfg_path, cfg_args={}):\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    performance_data = []\n",
    "\n",
    "    # init q_table with zeros\n",
    "    q_axis, q_table = init_q_table(env.observation_space, \n",
    "        env.action_space.n, cfg.discr, bool_position=cfg.bool_position, init_q=cfg.initial_q)\n",
    "\n",
    "    # begin training and testing\n",
    "    ep_reward_deque = deque([], maxlen=500)  # used to calculate the smoothed (avg over recent 500 episodes) ep_reward\n",
    "    for ep in range(cfg.train_episodes + 1):\n",
    "        # set epsilon value\n",
    "        if cfg.epsilon == 'glie':\n",
    "            epsilon = cfg.glie_b / (cfg.glie_b + ep)  \n",
    "        elif isinstance(cfg.epsilon, (int, float)):\n",
    "            epsilon = cfg.epsilon\n",
    "        else: \n",
    "            raise ValueError\n",
    "        \n",
    "        (state, _), done, ep_reward, timesteps = env.reset(), False, 0, 0\n",
    "        while not done:\n",
    "            action = get_action(state, q_axis, q_table, epsilon=epsilon) \n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "            q_table = update_q_value(state, action, new_state, cfg.gamma, reward, done, cfg.alpha,\n",
    "                                        q_axis, q_table)\n",
    "    \n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            timesteps += 1\n",
    "            if timesteps >= env.spec.max_episode_steps:\n",
    "                done = True\n",
    "                \n",
    "        ep_reward_deque.append(ep_reward)\n",
    "        info = {\n",
    "            'episode': ep,\n",
    "            'epsilon': epsilon,\n",
    "            'ep_reward': ep_reward,\n",
    "            'timesteps': timesteps,\n",
    "            'ep_reward_avg': np.mean(list(ep_reward_deque))\n",
    "        }\n",
    "\n",
    "        performance_data.append(info)\n",
    "        \n",
    "\n",
    "        if (not cfg.silent) and (ep % 500 == 0): print(info)\n",
    "    \n",
    "    work_dir = Path().cwd()/'results'/f'{cfg.env_name}'\n",
    "      \n",
    "    # save the q-value table and q_axis\n",
    "    file_pre = f\"{cfg.task_no}_\" if hasattr(cfg, 'task_no') else \"\"\n",
    "    file = file_pre + 'q_table_glie.pkl' if cfg.epsilon == 'glie' else file_pre + 'q_table_e.pkl'\n",
    "    u.save_object({'q_table': q_table, 'axis': q_axis},\n",
    "                        work_dir/file)\n",
    "    png_file = file_pre + '_glie.png' if cfg.epsilon == 'glie' else file_pre + '_e.png'\n",
    "    plot_training_data(performance_data, work_dir/png_file)\n",
    "    print('Training done!')\n",
    "\n",
    "    \n",
    "def test(cfg_path, cfg_args={'testing':True}):\n",
    "    \n",
    "    cfg_args.update({'testing':True})\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    \n",
    "    work_dir = Path().cwd()/'results'/f'{cfg.env_name}'\n",
    "    \n",
    "    # load q_table\n",
    "    file_pre = f\"{cfg.task_no}_\" if hasattr(cfg, 'task_no') else \"\"\n",
    "    file = file_pre + 'q_table_glie.pkl' if cfg.epsilon == 'glie' else file_pre + 'q_table_e.pkl'\n",
    "    data = u.load_object(work_dir/file)\n",
    "    q_axis, q_table = data['axis'], data['q_table']\n",
    "\n",
    "    # begin testing\n",
    "    for ep in range(cfg.test_episodes):\n",
    "        (state, _), done, ep_reward, timesteps = env.reset(), False, 0, 0\n",
    "        while not done:\n",
    "            action = get_action(state, q_axis, q_table, epsilon=0.0) # be greedy during testing\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            timesteps += 1\n",
    "        \n",
    "        info = {\n",
    "            'test_episode': ep,\n",
    "            'test_ep_reward': ep_reward,\n",
    "            'timesteps': timesteps,\n",
    "        }\n",
    "\n",
    "        if not cfg.silent: print(info)\n",
    "    \n",
    "    print('Testing done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329021c3-84d8-409b-9d48-d06383b912a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training with constant epsilon 0.1\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon=0.1, save_video=False)\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args) # < 30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acf49c-9f83-4363-b5e8-052214ef04e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "![plot_e](./results/CartPole-v1/task1__e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d603c2a-5582-4a7b-a04b-7b6ba03fc89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing with constant epsilon 0.1\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon=0.1, save_video=True)\n",
    "    test(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c840efe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad7d05103e53861c49975c1d2361488a",
     "grade": true,
     "grade_id": "cell-847ab0bf4eb63619",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea5fd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0564f2e3a7abff5b2e4abdb8d16932e",
     "grade": true,
     "grade_id": "cell-dd274e845abca949",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f6dd079-2502-4c2c-bd8e-0ac321b5826d",
   "metadata": {},
   "source": [
    "To visualize how the agent acts within the environment, execute the cell below. **Modify the path** to select the episode you'd like to observe. Remember, by default, videos are saved every 500 episodes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b9226-601e-4ba3-ae3b-05f5ea36d3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the results with constant epsilon 0.1\n",
    "if not skip_training:\n",
    "      video = Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex3-episode-3.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f187979-feb2-4833-a11a-68cfacc62080",
   "metadata": {},
   "source": [
    "\n",
    "**Important Reminder:** Before initiating the \"Training with GLIE\" process, make sure you've fully executed the steps under \"Training with a constant epsilon\". Be cautious, as the video save path remains identical; thus, any previous results will be overwritten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5da29e-f3f3-4b23-b879-a7786eb25c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = 2222\n",
    "\n",
    "# Training with GLIE\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon='glie', glie_b=b, save_video=False) # insert correct value for glie_b\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args) # < 30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1ee57-1cb1-4feb-9b19-de6d204aaffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "![plot_glie](./results/CartPole-v1/task1__glie.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77aea8d-2c8c-4e8f-baec-a3fd68fead5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing with GLIE\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon='glie', save_video=True)\n",
    "    test(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec32a0-acec-46fb-80cb-72bf8b6c5843",
   "metadata": {},
   "source": [
    "Just as in the \"visualize the results with constant epsilon 0.1\" section, **modify the path** to select the episode you'd like to observe. Remember, videos are saved every 500 episodes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6204544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing the results with GLIE\n",
    "if not skip_training:\n",
    "      video = Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex3-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13721787",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4729db952ef44314e885465232bd3c7b",
     "grade": true,
     "grade_id": "cell-a49c5defa1c9854f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c86e2e61",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Visualizing the Value Function (10 points) </h3> \n",
    "\n",
    "Use your Q-function values estimated with GLIE to calculate the optimal value function of each state. Complete the ```display_heatmap_vf()``` function below to plot the heatmap of the value function in terms of $x$ and $\\theta$, such that $\\theta$ is on horizontal and $x$ is on vertical axis. For plotting, average the values over $\\dot{x}$ and $\\dot{\\theta}$. Attach the heatmap in your report.\n",
    "\n",
    "**Hint:** For plotting the heatmap you can use Matplotlib-pyplot: ```pyplot.imshow(values_array)``` or Seaborn: ```seaborn.heatmap(values_array)```\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e7567c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "from pathlib import Path    # For handling filesystem paths in a more intuitive way\n",
    "\n",
    "# --- Third-party Libraries ---\n",
    "import matplotlib.pyplot as plt  # For creating static, interactive, and animated visualizations\n",
    "import seaborn as sns            # A data visualization library based on matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d18d44fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_heatmap_vf(env_name=\"CartPole-v1\", q_table_name='task1_q_table_glie.pkl'):\n",
    "    \"\"\"\n",
    "    Displays a heatmap of the value function for a given environment and Q-table.\n",
    "\n",
    "    Input:\n",
    "    - env_name (str): Name of the environment, default is \"CartPole-v1\".\n",
    "    - q_table_name (str): Name of the saved Q-table file, default is 'q_table_glie.pkl'.\n",
    "\n",
    "    Output:\n",
    "    - Visual display of the heatmap.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the Q-value array\n",
    "    work_dir = Path().cwd()/'results'/f'{env_name}'\n",
    "    \n",
    "    data = u.load_object(work_dir/q_table_name) # load q_table \n",
    "    q_axis, q_table = data['axis'], data['q_table'] \n",
    "    x_axis, th_axis = q_axis[0], q_axis[2]  # get the axis for x and \\theta\n",
    "\n",
    "    discr = q_table.shape[0] # get the number of discretized bins for a state variable in the Q-table\n",
    "    '''\n",
    "    # TODO: Task 2 Plot the heatmap of the value function\n",
    "    # 1. Compute the maximum Q-value for each state, which gives the value function of that state\n",
    "    # 2. Uses Seaborn and Matplotlib to display the heatmap.  \n",
    "    '''\n",
    "    \n",
    "    ########## Your code begins here. ##########\n",
    "    \n",
    "    table = np.zeros((discr, discr))\n",
    "    \n",
    "    for x in x_axis:\n",
    "        for th in th_axis:\n",
    "            idx = get_table_idx([x, th], (x_axis, th_axis))\n",
    "            table[idx] = np.max(q_table[idx])\n",
    "            \n",
    "    x_ticks = [ '%.2f' % elem for elem in x_axis ]\n",
    "    y_ticks = [ '%.2f' % elem for elem in th_axis ]\n",
    "            \n",
    "    plot = sns.heatmap(table, xticklabels=x_ticks, yticklabels=y_ticks)\n",
    "    plot.set_xlabel('x', fontsize=10)\n",
    "    plot.set_ylabel('theta', fontsize=10)\n",
    "    plot.set_title(\"Maximum state-values\")\n",
    "    \n",
    "    ########## Your code ends here. ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4bd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual display of the heatmap of the value function\n",
    "if not skip_training:\n",
    "    display_heatmap_vf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752c4c4-f909-41e0-8eae-10f80055dbc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f367a81d85146d63beb807550992c62b",
     "grade": false,
     "grade_id": "cell-88d5a81a7f1ca9f1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Do not remove this cell as it is used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba1c47",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.1</b> Analyzing the Value Function Heatmap (15 points) </h3> \n",
    "\n",
    "What do you think the heatmap of the Q-values would have looked like at different stages of training? Select all accurate description for each stage.\n",
    "\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8cdcf-f604-4889-992d-46498e5852b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4edd89e48f7b49d0ae8b49da370f0320",
     "grade": false,
     "grade_id": "cell-25b468ecabec94c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options\n",
    "\n",
    "1. Before training, the heatmap is all zeros (assuming Q-values are initialized as zeros)\n",
    "2. Before training, the heatmap has random values between -1 and 1\n",
    "3. After a single episode, the Q-values of visited states are updated with non-zero values\n",
    "4. After a single episode, there is a horizontal line in the middle of the heatmap\n",
    "5. After a single episode, the heatmap shows a vertical line in the middle of the heatmap\n",
    "6. Halfway through training, the heatmap already looks quite similar to the final optimal heatmap\n",
    "7. Halfway through training, the heatmap looks quite similar to the heatmap of after a single episode\n",
    "8. The cart's limited movement in the x direction and the pole's fall create a distinct pattern after one episode\n",
    "9. Random exploration in the first episode leads to scattered non-zero values throughout the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "784b9f86-27a9-4ab3-ac13-d49d83de414a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer question 2.1 with appropriate option numbers\n",
    "sq2_1 = [1, 3, 6, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1b80b-b12d-4667-a741-2193212d257c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b8c7b-1435-4cdd-adb5-d662b311d527",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ccc46773ce96275d3bcc395e75d607e",
     "grade": true,
     "grade_id": "cell-bb1012fd2db43f6a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd2c43-b826-44f2-9d3b-0405d81fe0c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "417a5911e76674561db02fbdd5b7b78d",
     "grade": true,
     "grade_id": "cell-11cd549484cdb161",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a9c61-4e3d-4c65-9bde-31a81ec2867e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f34f3b13b4c2a3fe4ca1f135730d1cf",
     "grade": true,
     "grade_id": "cell-d8599b1b7d33f525",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6173bf-6ecd-49e1-bbc5-dae35b1b6ab0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4ca7de9d9a4bf53f7f4e2fa172fd6b8",
     "grade": true,
     "grade_id": "cell-2d90b464f8c0a6c1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a4978-38b4-45fa-bb0c-b69f39b912ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed7234c17738c018ba5bd268dafb2a95",
     "grade": true,
     "grade_id": "cell-896fa45927fb7575",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87fb7b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3.</b> Investigating Initial Values (10 points) </h3> \n",
    "\n",
    "Set $\\epsilon$ to zero, effectively making the policy greedy w.r.t. current Q-value estimates. Run the training again while\n",
    "1) keeping the initial estimates of the Q function at 0,\n",
    "2) setting the initial estimates of the Q function to 50 for all states and actions.\n",
    "\n",
    "You can change the initial Q value estimates by passing ```initial_q=<value>``` in the following cells. \n",
    "\n",
    "**Attach training performance plots of both initializations in your report.**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf3dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Q function with a value of 0 and begin training\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon=0, initial_q=0, task_no='task3.1') # set the initial estimates of the Q function to 0\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee238f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual display of the heatmap of the value function\n",
    "if not skip_training:\n",
    "    display_heatmap_vf('CartPole-v1', 'task3.1_q_table_e.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc746ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Q function with a value of 50 and begin training\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon=0, initial_q=50,task_no='task3.2') # set the initial estimates of the Q function to 50\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed3aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual display of the heatmap of the value function\n",
    "if not skip_training:\n",
    "    display_heatmap_vf('CartPole-v1', 'task3.2_q_table_e.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09514150",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9ab1fd48e4583a37a4c80bea1298d51",
     "grade": true,
     "grade_id": "cell-f3641856fbc277ec",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d54874-22c9-4367-a262-10399f81c0a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "620e26ec411953c00ef7617e63c34d79",
     "grade": true,
     "grade_id": "cell-1f7e70d287bd137d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbea9f8b",
   "metadata": {},
   "source": [
    "Based on the results you observed in Task 3, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97da87",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.1</b> Analyzing Initial Values (5 points) </h3> \n",
    "\n",
    "In which case does the model perform better?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb220423-a43b-4bd0-a74f-08dc8b992895",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d749a507f2d6d209cb694f7f837870e5",
     "grade": false,
     "grade_id": "cell-a8f1edecd8ad4cf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options\n",
    "1. The model performs better with initial Q-values of 0\n",
    "2. The model performs better with initial Q-values of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "186bc4b0-0956-41b9-b0d8-4c2ca2702010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 3.1 with appropriate option number\n",
    "sq3_1 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d749faf-4b6a-4aaa-9391-385676023219",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de696f-a080-4681-a4fc-adfa09f8290e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "275c853e31cbf78906be57b98ca1fd90",
     "grade": true,
     "grade_id": "cell-4ea4b3ce9d0c8a9d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a200626",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.2</b> Exploration (15 points) </h3> \n",
    "\n",
    "Why is this the case, and how does the initialization of Q values affect exploration?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505f300-8926-49da-9d75-efcff37e36d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efdab8492c8f43508387a5f41f0c1f28",
     "grade": false,
     "grade_id": "cell-5d5a551743c4c795",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options\n",
    "\n",
    "1. Higher initial Q-values encourage exploration by making unexplored actions seem more attractive.\n",
    "2. Q-values of 0 make the agent indifferent to all actions initially, leading to diverse exploration.\n",
    "3. Initializing Q-values to 50 creates an \"optimistic\" prior that encourages trying new actions.\n",
    "4. Q-values of 0 promote exploration because the agent needs to try all actions to differentiate their values.\n",
    "5. Higher initial Q-values allow the agent to overcome local optima more easily.\n",
    "6. Initializing to 0 encourages exploration as the agent is more willing to try actions with unknown outcomes.\n",
    "7. Q-values of 0 cause the agent to be overly cautious, avoiding potentially risky but rewarding actions.\n",
    "8. Higher initial Q-values compensate for the lack of explicit exploration in a greedy policy.\n",
    "9. Initializing to 0 promotes a \"blank slate\" approach, allowing the agent to explore without bias.\n",
    "10. With Q-values initialized to 0, the agent tends to stick with the first action it tries due to non-negative rewards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deeda856-b1b9-4846-83ad-f3300ab01172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 3.2 with appropriate option numbers\n",
    "sq3_2 = [1, 3, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c51db4-8115-4ed9-b6c6-bea755169295",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3cfa3c-4288-4737-b146-3e6b2b2c4812",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e17ad844248d51df28cd57f9355952d",
     "grade": true,
     "grade_id": "cell-d2fc4c748d4c501f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d7567-3414-4570-a070-def2d5bc5f93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4be9b3f2ea3ba051fe031309d036d678",
     "grade": true,
     "grade_id": "cell-c8e8a3fb8886bcf8",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1646f-e247-46fb-bb5c-3e9027d41340",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a3b206426459f462c2faecbd69e1d58",
     "grade": true,
     "grade_id": "cell-b335c5b754e65353",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd959222-2cf3-4553-ad7d-b354e805a92e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b391c37af6f5a1e6afcda83d42141f71",
     "grade": true,
     "grade_id": "cell-ed54ceb0160e9dae",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6176d62f",
   "metadata": {},
   "source": [
    "# 3. Lunar lander <a id='3.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724c44",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/lunar_lander.png\" width=\"600px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: The Lunar lander environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The ***Lunar lander*** environment is shown in Figure 2. The goal is to make the lunar lander land on the ground between two flag poles. The agent receives a positive reward for moving towards the landing pad, landing, etc. A negative reward is given for firing the main engine (more fuel-efficient policies are better) and for crashing. Four actions are available: firing the left/right/main engines, or doing nothing (free fall). The observation vector consists of 6 continuous and 2 discrete values:\n",
    "\n",
    "$$\n",
    "o=\\left(\\begin{array}{llllllll}\n",
    "x & y & \\dot{x} & \\dot{y} & \\theta & \\dot{\\theta} & c_l & c_r\n",
    "\\end{array}\\right)^T,\n",
    "$$\n",
    "\n",
    "where $x$ and $y$ are the coordinates of the lander, $\\dot{x}$ and $\\dot{y}$ its velocities, $\\theta$ represents the rotation angle and $\\dot{\\theta}$ the angular velocity of the lander. Two discrete values $c_l$ and $c_r$  indicate whether the lander‚Äôs legs are in contact with the ground (0 or 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636f10e",
   "metadata": {},
   "source": [
    "<a id='T4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 4.</b> Using Q-Learning on the Lunar Lander Environment (5 points) </h3> \n",
    "\n",
    "Run the training for Lunar Lander environment by executing cell ```Training on the Lunar Lander Environment```. Run it for 20000 episodes (which was enough for the Cartpole to learn). **Attach the training performance plot in your submission.**\n",
    "\n",
    "**Heads Up:** The generated file `task4_q_table.pkl` is sizeable, approximately 2 GB. Ensure you **delete** it before submitting. But do not delete the `task4__glie.png` file.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec43485e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training on the Lunar Lander Environment\n",
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon='glie', glie_b=b, save_video=False, discr=12) # insert correct value for glie_b\n",
    "    train(cfg_path=Path().cwd()/'cfg'/'lunarlander_v2.yaml', cfg_args=cfg_args) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d4481f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    cfg_args=dict(epsilon='glie', save_video=True, test_episodes=5)\n",
    "    test(cfg_path=Path().cwd()/'cfg'/'lunarlander_v2.yaml', cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05937429-abca-4681-b88a-f5f3f63eb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting Q-table due to its size as it's not needed for grading.\n",
    "q_table_path = Path().cwd()/'results'/'LunarLander-v2'/'task4_q_table_glie.pkl'\n",
    "if os.path.exists(q_table_path):\n",
    "    os.remove(q_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2f33113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(Path().cwd()/'results'/'LunarLander-v2'/'video'/'test'/'ex3-episode-3.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0a252",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "650c9f3243d0a1d2956190dcf3759aa0",
     "grade": true,
     "grade_id": "cell-d29073ccaa4d44b2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39cd6a58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a0f5b818b10379e790e0b2d75e9484d",
     "grade": false,
     "grade_id": "cell-a471f8bfee9e6a17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Lunar Lander Performance (15 points) </h3> \n",
    "\n",
    "Why doesn't the Lunar Lander learn to land consistently between the flag poles after 20,000 episodes? Select all that apply.\n",
    "\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd97212-ad22-4953-9a22-35d8133b9738",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2574c891d50448fac57514f8f74d7c6",
     "grade": false,
     "grade_id": "cell-908548faad54e815",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options\n",
    "1. The lunar lander's dynamics are too complex for a simple Q-learning approach\n",
    "2. The discretization of continuous states causes loss of crucial information\n",
    "3. The Q-learning algorithm struggles with high-dimensional state spaces\n",
    "4. The reward function may not provide enough guidance for precise landing\n",
    "5. The epsilon-greedy strategy may not provide sufficient exploration for Q-learning\n",
    "6. 20,000 episodes is insufficient for the Q-learning algorithm to converge in lunar lander\n",
    "7. The state space is too large for efficient exploration\n",
    "8. The combination of continuous state space and discrete action space is challenging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7e70b5a-f9df-4ca7-bb0f-c42937027d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer question 4.2 with appropriate option numbers\n",
    "sq4_2 = [1, 2, 3, 4, 6, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8fc54-59b1-4ecc-b56f-bab48c1d2964",
   "metadata": {},
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e71fd-4f1a-4087-afbe-6a52c162832b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4acdc343437df35b85f1eb5bfd75de1",
     "grade": true,
     "grade_id": "cell-40dbcf45474410e7",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d0d12-166c-4c4c-ae24-1d10d4488b3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1eed6ae7a6b2fd0e983742243c77a37a",
     "grade": true,
     "grade_id": "cell-6667bcd9d943e6f3",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61efda9-0a20-4005-99f9-8cfe30fe0874",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4d89233545978970fc3c3fd99372c4a",
     "grade": true,
     "grade_id": "cell-13d4526e844ef54e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82da6a3-4931-4749-a8ef-eb24d190b917",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c96eabd6af98e4108d2b2ccd70cd94ef",
     "grade": true,
     "grade_id": "cell-a705852648c0ce3b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98e534eb",
   "metadata": {},
   "source": [
    "# 4. Submitting <a id='4.'></a>\n",
    "Ensure all tasks and questions (in ```ex3.ipynb```) are answered and the relevant plots are recorded in the relevant places. Details about attaching images and figures can be found below. The relevant graphs to be included for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `task1_e.png`: Cartpole, constant value of $\\epsilon$ training performance plots in terms of episode and smoothed episodic reward\n",
    "  - `task1_glie.png`: Cartpole, GLIE training performance plots in terms of episode and smoothed episodic reward\n",
    "  - `task3.1_e.png`: Cartpole, training performance plots of the initial estimates of the Q function at 0 \n",
    "  - `task3.2_e.png`: Cartpole, training performance plots of the initial estimates of the Q function at 50\n",
    "  - `task4_glie.png`: Luner Lander, training performance plots in terms of episode and smoothed episodic reward\n",
    "- Q-table files:\n",
    "  - `task1_q_table_e.pkl`: Cartpole, Q-table for the constant exploration rate.\n",
    "  - `task1_q_table_glie.pkl`: Cartpole, Q-table for GLIE.\n",
    "\n",
    "\n",
    "Ensure the correct model files and plots are saved:\n",
    "- ```results/CartPole-v1/task1_e.png``` From Task 1\n",
    "- ```results/CartPole-v1/task1_glie.png``` From Task 1\n",
    "- ```results/CartPole-v1/task1_q_table_e.pkl``` From Task 1\n",
    "- ```results/CartPole-v1/task1_q_table_glie.pkl``` From Task 1\n",
    "- ```results/CartPole-v1/task3.1__e.png``` From Task 3\n",
    "- ```results/CartPole-v1/task3.2__e.png``` From Task 3\n",
    "- ```results/LunarLander-v2/task4__glie.png``` From Task 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15c6c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3823",
   "metadata": {},
   "source": [
    "## 4.1 Feedback <a id='4.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19aa8d-6ce6-4688-b6f8-db0ae3caa596",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7c8370-ff84-4dc9-aab8-596782c55d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d254d1-ad16-4c10-87a6-72afb79d49fe",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58193777-64de-4d7c-be84-249d34e1c027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = 3   # Student Task 1. Implementing Q-Learning (25 points)\n",
    "T2 = 2   # Student Task 2. Visualizing the Value Function (10 points)\n",
    "Q2_1 = 3 # Student Question 2.1 Analyzing the Value Function Heatmap (15 points)\n",
    "T3 = 1   # Student Task 3. Investigating Initial Values (10 points)\n",
    "Q3_1 = 1 # Student Question 3.1 Analyzing Initial Values (5 points)\n",
    "Q3_2 = 4 # Student Question 3.2 Exploration (15 points)\n",
    "T4 = 1   # Student Task 4. Using Q-Learning on the Lunar Lander Environment (5 points)\n",
    "Q4_1 = 4 # Student Question 4.1 Lunar Lander Performance (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af146c4d-9475-4c0b-8b43-823fc345a584",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f5a460a-eb5b-45e4-a091-7153416dbd43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = 4   # Student Task 1. Implementing Q-Learning (25 points)\n",
    "T2 = 5   # Student Task 2. Visualizing the Value Function (10 points)\n",
    "Q2_1 = 3 # Student Question 2.1 Analyzing the Value Function Heatmap (15 points)\n",
    "T3 = 3   # Student Task 3. Investigating Initial Values (10 points)\n",
    "Q3_1 = 3 # Student Question 3.1 Analyzing Initial Values (5 points)\n",
    "Q3_2 = 2 # Student Question 3.2 Exploration (15 points)\n",
    "T4 = 4   # Student Task 4. Using Q-Learning on the Lunar Lander Environment (5 points)\n",
    "Q4_1 = 3 # Student Question 4.1 Lunar Lander Performance (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4ebe0-4012-42fd-90ae-2c8e386afa4f",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c9543-3652-4311-ad4d-6b6a087e5587",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
